{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d41e715-3644-42de-b12a-cb82d5bce37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in /home/jupyter/.local/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components>2 in /home/jupyter/.local/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.10/site-packages (1.34.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Obtaining dependency information for google-cloud-aiplatform from https://files.pythonhosted.org/packages/f6/67/734b8c73b8e708a24301b8a0a072ddfe936816896d12af4884e4f7bbd3b0/google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.15)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (1.34.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.10.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.0.1)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (26.1.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (3.20.3)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (1.26.16)\n",
      "Requirement already satisfied: Jinja2==3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components>2) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2==3.1.2->google-cloud-pipeline-components>2) (2.1.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.60.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.57.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (1.16.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (2023.7.22)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp>2) (68.1.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp>2) (1.6.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp>2) (1.3.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp>2) (3.2.2)\n",
      "Downloading google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.34.0\n",
      "    Uninstalling google-cloud-aiplatform-1.34.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.34.0\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.35.0\n",
      "KFP SDK version: 2.3.0\n",
      "google-cloud-aiplatform==1.35.0\n",
      "google_cloud_pipeline_components version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform\n",
    "\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "970bf541-efa7-40d3-bd6e-99a386f874d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    Metrics,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    OutputPath,\n",
    "    InputPath,\n",
    ")\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417fa2c-3a75-4a1f-9a3c-4b1d05fd5d2f",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "561d6e44-ddf9-4a48-9c23-fdaa2cd29223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"de23-398309\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://data_de2023_2065718\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ec31a-d853-4770-b2c5-0d609d98995e",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a3c216d-39bb-47bb-af47-29e021aa9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    ")\n",
    "def download_data(\n",
    "    project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"download data\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Downloaing the file from a google bucket\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info(\"Downloaded Data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78d4fc-377b-4a25-a53b-dbdb54ce0a03",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f4ce2ca-8986-4116-85ac-569e9d9cbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(\n",
    "    dataset: Input[Dataset],\n",
    "    dataset_train_X: Output[Dataset],\n",
    "    dataset_test_X: Output[Dataset],\n",
    "    dataset_train_y: Output[Dataset],\n",
    "    dataset_test_y: Output[Dataset],\n",
    "    level: float,\n",
    "):\n",
    "    \"\"\"train_test_split\"\"\"\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    alldata = pd.read_csv(dataset.path + \".csv\", index_col=None)\n",
    "    X_train, X_test, y_train, y_test = tts(\n",
    "        alldata.drop(\"quality\", axis=1),\n",
    "        alldata[\"quality\"],\n",
    "        test_size=level,\n",
    "        random_state=6,\n",
    "    )\n",
    "    X_train.to_csv(dataset_train_X.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    X_test.to_csv(dataset_test_X.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    y_train.to_csv(dataset_train_y.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    y_test.to_csv(dataset_test_y.path + \".csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1c694-12e1-4581-b3ac-ea8c94380527",
   "metadata": {},
   "source": [
    "#### Pipeline Component : PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87f7a8e8-a834-48a3-b4cb-d8870103ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def PCA(\n",
    "    standard_features: Input[Dataset],\n",
    "    pca_features: Output[Dataset],\n",
    "    scaler_model: Output[Model],\n",
    "    pca_model: Output[Model],\n",
    "):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    features = pd.read_csv(standard_features.path + \".csv\")\n",
    "    train_features = features.drop(\"quality\", axis=1)\n",
    "    test_features = features[[\"quality\"]]\n",
    "    scaler.fit(train_features)\n",
    "    scaled_features = pd.DataFrame(\n",
    "        scaler.transform(train_features),\n",
    "        columns=train_features.columns,\n",
    "        index=train_features.index,\n",
    "    )\n",
    "\n",
    "    pca = PCA(n_components=0.1, svd_solver=\"full\")\n",
    "    pca.fit(scaled_features)\n",
    "    pca_df = pd.DataFrame(pca.transform(scaled_features))\n",
    "    result = pd.concat([pca_df, test_features], axis=1)\n",
    "\n",
    "    result.to_csv(pca_features.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Save the scaler\n",
    "    s_file = scaler_model.path + \".pkl\"\n",
    "    with open(s_file, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "        # Save the scaler\n",
    "    p_file = pca_model.path + \".pkl\"\n",
    "    with open(p_file, \"wb\") as f:\n",
    "        pickle.dump(pca, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b8dd4-6b1c-4cca-ac6a-e3aa2f30dfb5",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdd1aa34-7b2d-45f6-ae24-12fa8cfcb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_rf(\n",
    "    train_features_X: Input[Dataset],\n",
    "    test_features_X: Input[Dataset],\n",
    "    train_features_y: Input[Dataset],\n",
    "    test_features_y: Input[Dataset],\n",
    "    out_model: Output[Model],\n",
    ") -> NamedTuple(\"outputs\", metrics=dict):\n",
    "    \"\"\" \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "        mean_absolute_error,\n",
    "    )\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    X_train = pd.read_csv(train_features_X.path + \".csv\")\n",
    "    y_train = pd.read_csv(train_features_y.path + \".csv\")\n",
    "\n",
    "    logging.info(X_train.columns)\n",
    "\n",
    "    parameters = {\n",
    "        \"n_estimators\": [100, 250],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 5, 10],\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=6)\n",
    "    rf_gs = GridSearchCV(rf, parameters)\n",
    "    rf_gs.fit(X_train, y_train)\n",
    "    best_params = rf_gs.best_params_\n",
    "\n",
    "    X_test = pd.read_csv(test_features_X.path + \".csv\")\n",
    "    y_test = pd.read_csv(test_features_y.path + \".csv\")\n",
    "\n",
    "    # Predicting Test Set\n",
    "\n",
    "    y_pred = rf_gs.predict(X_test)\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    logging.info(metrics_dict)\n",
    "\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"rf_gs\"\n",
    "    out_model.metadata[\"best_params\"] = best_params\n",
    "\n",
    "    # Save the model\n",
    "    m_file = out_model.path + \".pkl\"\n",
    "    with open(m_file, \"wb\") as f:\n",
    "        pickle.dump(rf_gs, f)\n",
    "\n",
    "    outputs = NamedTuple(\"outputs\", metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac202838-a61d-4806-ae58-3ea2dc79760e",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-GBC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e81315be-31d9-4b63-8105-ad26b3bfa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_gbc(\n",
    "    train_features_X: Input[Dataset],\n",
    "    test_features_X: Input[Dataset],\n",
    "    train_features_y: Input[Dataset],\n",
    "    test_features_y: Input[Dataset],\n",
    "    out_model: Output[Model],\n",
    ") -> NamedTuple(\"outputs\", metrics=dict):\n",
    "    \"\"\" \"\"\"\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "        mean_absolute_error,\n",
    "    )\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    X_train = pd.read_csv(train_features_X.path + \".csv\")\n",
    "    y_train = pd.read_csv(train_features_y.path + \".csv\")\n",
    "\n",
    "    logging.info(X_train.columns)\n",
    "\n",
    "    parameters = {\n",
    "        \"n_estimators\": [100, 250],\n",
    "        \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 5, 10],\n",
    "    }\n",
    "\n",
    "    gbc = GradientBoostingClassifier(random_state=6)\n",
    "    gbc_gs = GridSearchCV(gbc, parameters)\n",
    "    gbc_gs.fit(X_train, y_train)\n",
    "    best_params = gbc_gs.best_params_\n",
    "\n",
    "    X_test = pd.read_csv(test_features_X.path + \".csv\")\n",
    "    y_test = pd.read_csv(test_features_y.path + \".csv\")\n",
    "\n",
    "    # Predicting Test Set\n",
    "\n",
    "    y_pred = gbc_gs.predict(X_test)\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    logging.info(metrics_dict)\n",
    "\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"gbc_gs\"\n",
    "    out_model.metadata[\"best_params\"] = best_params\n",
    "\n",
    "    # Save the model\n",
    "    m_file = out_model.path + \".pkl\"\n",
    "    with open(m_file, \"wb\") as f:\n",
    "        pickle.dump(gbc_gs, f)\n",
    "\n",
    "    outputs = NamedTuple(\"outputs\", metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06696dbd",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc60261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_rf(\n",
    "    model: Input[Model],\n",
    "    features: Input[Dataset],\n",
    "    scaler_model: Input[Model],\n",
    "    pca_model: Input[Model],\n",
    "    results: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "\n",
    "    # Loading the saved model\n",
    "    model_rf = pickle.load(open(model.path + \".pkl\", \"rb\"))\n",
    "    scaler = pickle.load(open(scaler_model.path + \".pkl\", \"rb\"))\n",
    "    pca = pickle.load(open(pca_model.path + \".pkl\", \"rb\"))\n",
    "\n",
    "    X_test = df.drop(\"quality\", axis=1)\n",
    "\n",
    "    # transform the test data\n",
    "    scaled_features = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index,\n",
    "    )\n",
    "    X_test_pca = pd.DataFrame(pca.transform(scaled_features))\n",
    "\n",
    "    df_complete = df.copy()\n",
    "    y_pred = model_rf.predict(X_test_pca)\n",
    "    logging.info(y_pred)\n",
    "    df_complete[\"pclass\"] = y_pred.tolist()\n",
    "    df_complete.to_csv(results.path + \".csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91997764",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39adb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_gbc(\n",
    "    model: Input[Model],\n",
    "    features: Input[Dataset],\n",
    "    scaler_model: Input[Model],\n",
    "    pca_model: Input[Model],\n",
    "    results: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "\n",
    "    # Loading the saved model\n",
    "    model_gbc = pickle.load(open(model.path + \".pkl\", \"rb\"))\n",
    "    scaler = pickle.load(open(scaler_model.path + \".pkl\", \"rb\"))\n",
    "    pca = pickle.load(open(pca_model.path + \".pkl\", \"rb\"))\n",
    "\n",
    "    X_test = df.drop(\"quality\", axis=1)\n",
    "\n",
    "    # transform the test data\n",
    "    scaled_features = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index,\n",
    "    )\n",
    "\n",
    "    X_test_pca = pd.DataFrame(pca.transform(scaled_features))\n",
    "\n",
    "    df_complete = df.copy()\n",
    "    y_pred = model_gbc.predict(X_test_pca)\n",
    "    logging.info(y_pred)\n",
    "    df_complete[\"pclass\"] = y_pred.tolist()\n",
    "    df_complete.to_csv(results.path + \".csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92339315",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "905da275",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10.7-slim\")\n",
    "def compare_model(rf_metrics: dict, gbc_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(rf_metrics)\n",
    "    logging.info(gbc_metrics)\n",
    "    if rf_metrics.get(\"f1_score\") > gbc_metrics.get(\"f1_score\"):\n",
    "        return \"rf_gs\"\n",
    "    else:\n",
    "        return \"gbc_gs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d8cb7",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aaefacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    \"\"\"upload model to gcs\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob(\n",
    "        str(model.metadata[\"algo\"]) + \"_model\" + str(model.metadata[\"file_type\"])\n",
    "    )\n",
    "    blob.upload_from_filename(model.path + str(model.metadata[\"file_type\"]))\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f21478",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0ed4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(name=\"winequality-predictor-training-pipeline-v1\")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    data_bucket: str,\n",
    "    dataset_filename: str,\n",
    "    model_repo: str,\n",
    "    prediction_set: str,\n",
    "):\n",
    "    di_op = download_data(\n",
    "        project_id=project_id, bucket=data_bucket, file_name=dataset_filename\n",
    "    )\n",
    "\n",
    "    pca_job_run_op = PCA(standard_features=di_op.outputs[\"dataset\"])\n",
    "\n",
    "    tts_job_run_op = train_test_split(\n",
    "        dataset=pca_job_run_op.outputs[\"pca_features\"], level=0.2\n",
    "    )\n",
    "\n",
    "    training_rf_job_run_op = train_rf(\n",
    "        train_features_X=tts_job_run_op.outputs[\"dataset_train_X\"],\n",
    "        train_features_y=tts_job_run_op.outputs[\"dataset_train_y\"],\n",
    "        test_features_X=tts_job_run_op.outputs[\"dataset_test_X\"],\n",
    "        test_features_y=tts_job_run_op.outputs[\"dataset_test_y\"],\n",
    "    )\n",
    "\n",
    "    training_gbc_job_run_op = train_gbc(\n",
    "        train_features_X=tts_job_run_op.outputs[\"dataset_train_X\"],\n",
    "        train_features_y=tts_job_run_op.outputs[\"dataset_train_y\"],\n",
    "        test_features_X=tts_job_run_op.outputs[\"dataset_test_X\"],\n",
    "        test_features_y=tts_job_run_op.outputs[\"dataset_test_y\"],\n",
    "    )\n",
    "\n",
    "    comp_model__op = compare_model(\n",
    "        rf_metrics=training_rf_job_run_op.outputs[\"metrics\"],\n",
    "        gbc_metrics=training_gbc_job_run_op.outputs[\"metrics\"],\n",
    "    ).after(training_rf_job_run_op, training_gbc_job_run_op)\n",
    "\n",
    "    prediction_set_op = download_data(\n",
    "        project_id=project_id, bucket=data_bucket, file_name=prediction_set\n",
    "    )\n",
    "\n",
    "    # defining the branching condition\n",
    "    with dsl.If(comp_model__op.output == \"rf_gs\"):\n",
    "        predict_rf_job_run_op = predict_rf(\n",
    "            model=training_rf_job_run_op.outputs[\"out_model\"],\n",
    "            features=prediction_set_op.outputs[\"dataset\"],\n",
    "            scaler_model=pca_job_run_op.outputs[\"scaler_model\"],\n",
    "            pca_model=pca_job_run_op.outputs[\"pca_model\"],\n",
    "        )\n",
    "        upload_model_rf_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_rf_job_run_op.outputs[\"out_model\"],\n",
    "        ).after(predict_rf_job_run_op)\n",
    "\n",
    "    with dsl.If(comp_model__op.output == \"gbc_gs\"):\n",
    "        predict_gbc_job_run_op = predict_gbc(\n",
    "            model=training_gbc_job_run_op.outputs[\"out_model\"],\n",
    "            features=prediction_set_op.outputs[\"dataset\"],\n",
    "            scaler_model=pca_job_run_op.outputs[\"scaler_model\"],\n",
    "            pca_model=pca_job_run_op.outputs[\"pca_model\"],\n",
    "        )\n",
    "        upload_model_gbc_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_gbc_job_run_op.outputs[\"out_model\"],\n",
    "        ).after(predict_gbc_job_run_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c98eac",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1ea003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"winequality_predictor_training_pipeline_v1.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff31d24",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe1131c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/winequality-predictor-training-pipeline-v1-20231012150943?project=915562509454\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/915562509454/locations/us-central1/pipelineJobs/winequality-predictor-training-pipeline-v1-20231012150943\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"winequality-predictor-v1\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"winequality_predictor_training_pipeline_v1.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,  # makesure to use your project id\n",
    "        \"data_bucket\": \"data_de2023_2065718\",  # makesure to use your data bucket name\n",
    "        \"dataset_filename\": \"train_data.csv\",  # makesure to upload these to your data bucket from DE2023\n",
    "        \"model_repo\": \"models_de2023_2065718\",  # makesure to use your model bucket name\n",
    "        \"prediction_set\": \"predict_data.csv\",  # makesure to use your data bucket name\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdc6ab-6acc-4e9a-a40c-398d58619e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
