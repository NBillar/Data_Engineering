# PIPELINE DEFINITION
# Name: winequality-predictor-training-pipeline-v1
# Inputs:
#    data_bucket: str
#    dataset_filename: str
#    model_repo: str
#    prediction_set: str
#    project_id: str
components:
  comp-compare-model:
    executorLabel: exec-compare-model
    inputDefinitions:
      parameters:
        gbc_metrics:
          parameterType: STRUCT
        mlp_metrics:
          parameterType: STRUCT
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-condition-1:
    dag:
      tasks:
        predict-mlp:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-predict-mlp
          inputs:
            artifacts:
              features:
                componentInputArtifact: pipelinechannel--download-data-2-dataset
              model:
                componentInputArtifact: pipelinechannel--train-mlp-out_model
              pca_model:
                componentInputArtifact: pipelinechannel--pca-pca_model
              scaler_model:
                componentInputArtifact: pipelinechannel--pca-scaler_model
          taskInfo:
            name: predict-mlp
        upload-model-to-gcs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs
          dependentTasks:
          - predict-mlp
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-mlp-out_model
            parameters:
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs
    inputDefinitions:
      artifacts:
        pipelinechannel--download-data-2-dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        pipelinechannel--pca-pca_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pipelinechannel--pca-scaler_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pipelinechannel--train-mlp-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-model-Output:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-condition-2:
    dag:
      tasks:
        predict-gbc:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-predict-gbc
          inputs:
            artifacts:
              features:
                componentInputArtifact: pipelinechannel--download-data-2-dataset
              model:
                componentInputArtifact: pipelinechannel--train-gbc-out_model
              pca_model:
                componentInputArtifact: pipelinechannel--pca-pca_model
              scaler_model:
                componentInputArtifact: pipelinechannel--pca-scaler_model
          taskInfo:
            name: predict-gbc
        upload-model-to-gcs-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs-2
          dependentTasks:
          - predict-gbc
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-gbc-out_model
            parameters:
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs-2
    inputDefinitions:
      artifacts:
        pipelinechannel--download-data-2-dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        pipelinechannel--pca-pca_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pipelinechannel--pca-scaler_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pipelinechannel--train-gbc-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-model-Output:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-download-data:
    executorLabel: exec-download-data
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        file_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-download-data-2:
    executorLabel: exec-download-data-2
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        file_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-pca:
    executorLabel: exec-pca
    inputDefinitions:
      artifacts:
        standard_features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        pca_features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        pca_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-predict-gbc:
    executorLabel: exec-predict-gbc
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pca_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        results:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-predict-mlp:
    executorLabel: exec-predict-mlp
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        pca_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        results:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-gbc:
    executorLabel: exec-train-gbc
    inputDefinitions:
      artifacts:
        test_features_X:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        test_features_y:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_features_X:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_features_y:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-mlp:
    executorLabel: exec-train-mlp
    inputDefinitions:
      artifacts:
        test_features_X:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        test_features_y:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_features_X:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_features_y:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-test-split:
    executorLabel: exec-train-test-split
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        level:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        dataset_test_X:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_test_y:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_train_X:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_train_y:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-upload-model-to-gcs:
    executorLabel: exec-upload-model-to-gcs
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-upload-model-to-gcs-2:
    executorLabel: exec-upload-model-to-gcs-2
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-compare-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compare_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compare_model(mlp_metrics: dict, gbc_metrics: dict) -> str:\n\
          \    import logging\n    import json\n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n    logging.info(mlp_metrics)\n    logging.info(gbc_metrics)\n\
          \    if mlp_metrics.get(\"mae\") < gbc_metrics.get(\"mae\"):\n        return\
          \ \"mlp_gs\"\n    else:\n        return \"gbc_gs\"\n\n"
        image: python:3.10.7-slim
    exec-download-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_data(\n    project_id: str, bucket: str, file_name:\
          \ str, dataset: Output[Dataset]\n):\n    \"\"\"download data\"\"\"\n   \
          \ from google.cloud import storage\n    import pandas as pd\n    import\
          \ logging\n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    # Downloaing the file from a google bucket\n\
          \    client = storage.Client(project=project_id)\n    bucket = client.bucket(bucket)\n\
          \    blob = bucket.blob(file_name)\n    blob.download_to_filename(dataset.path\
          \ + \".csv\")\n    logging.info(\"Downloaded Data!\")\n\n"
        image: python:3.10.7-slim
    exec-download-data-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_data(\n    project_id: str, bucket: str, file_name:\
          \ str, dataset: Output[Dataset]\n):\n    \"\"\"download data\"\"\"\n   \
          \ from google.cloud import storage\n    import pandas as pd\n    import\
          \ logging\n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    # Downloaing the file from a google bucket\n\
          \    client = storage.Client(project=project_id)\n    bucket = client.bucket(bucket)\n\
          \    blob = bucket.blob(file_name)\n    blob.download_to_filename(dataset.path\
          \ + \".csv\")\n    logging.info(\"Downloaded Data!\")\n\n"
        image: python:3.10.7-slim
    exec-pca:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - PCA
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'google-cloud-storage' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef PCA(\n    standard_features: Input[Dataset],\n    pca_features:\
          \ Output[Dataset],\n    scaler_model: Output[Model],\n    pca_model: Output[Model],\n\
          \    project_id: str, \n    model_repo: str,\n):\n    \"\"\" \"\"\"\n  \
          \  from google.cloud import storage\n    from sklearn.decomposition import\
          \ PCA\n    from sklearn.preprocessing import StandardScaler\n\n    import\
          \ pandas as pd\n    import json\n    import logging\n    import sys\n  \
          \  import os\n    import pickle\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    scaler = StandardScaler()\n\n    features =\
          \ pd.read_csv(standard_features.path + \".csv\").drop('Unnamed: 0', axis\
          \ = 1)\n    train_features = features.drop(\"quality\", axis=1)\n    test_features\
          \ = features[[\"quality\"]]\n    scaler.fit(train_features)\n    scaled_features\
          \ = pd.DataFrame(\n        scaler.transform(train_features),\n        columns=train_features.columns,\n\
          \        index=train_features.index,\n    )\n\n    pca = PCA(n_components=0.1,\
          \ svd_solver=\"full\")\n    pca.fit(scaled_features)\n    pca_df = pd.DataFrame(pca.transform(scaled_features))\n\
          \    result = pd.concat([pca_df, test_features], axis=1)\n\n    result.to_csv(pca_features.path\
          \ + \".csv\", index=False, encoding=\"utf-8-sig\")\n\n    # Save the scaler\n\
          \    s_file = scaler_model.path + \".pkl\"\n    with open(s_file, \"wb\"\
          ) as f:\n        pickle.dump(scaler, f)\n\n        # Save the scaler\n \
          \   p_file = pca_model.path + \".pkl\"\n    with open(p_file, \"wb\") as\
          \ f:\n        pickle.dump(pca, f)\n\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob1 = bucket.blob(\n    \
          \    'scaler_model' + \".pkl\"\n    )\n    blob1.upload_from_filename(s_file)\n\
          \n    blob2 = bucket.blob(\n        'pca_model' + \".pkl\"\n    )\n    blob2.upload_from_filename(p_file)\n\
          \n"
        image: python:3.10.7-slim
    exec-predict-gbc:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - predict_gbc
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef predict_gbc(\n    model: Input[Model],\n    features: Input[Dataset],\n\
          \    scaler_model: Input[Model],\n    pca_model: Input[Model],\n    results:\
          \ Output[Dataset],\n):\n    import pandas as pd\n    import pickle\n   \
          \ import json\n    import logging\n    import sys\n    import os\n\n   \
          \ logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    df =\
          \ pd.read_csv(features.path + \".csv\").drop('Unnamed: 0', axis = 1)\n\n\
          \    # Loading the saved model\n    model_gbc = pickle.load(open(model.path\
          \ + \".pkl\", \"rb\"))\n    scaler = pickle.load(open(scaler_model.path\
          \ + \".pkl\", \"rb\"))\n    pca = pickle.load(open(pca_model.path + \".pkl\"\
          , \"rb\"))\n\n    X_test = df.drop(\"quality\", axis=1)\n\n    # transform\
          \ the test data\n    scaled_features = pd.DataFrame(\n        scaler.transform(X_test),\n\
          \        columns=X_test.columns,\n        index=X_test.index,\n    )\n\n\
          \    X_test_pca = pd.DataFrame(pca.transform(scaled_features))\n\n    df_complete\
          \ = df.copy()\n    y_pred = model_gbc.predict(X_test_pca)\n    logging.info(y_pred)\n\
          \    df_complete[\"pclass\"] = y_pred.tolist()\n    df_complete.to_csv(results.path\
          \ + \".csv\", index=False, encoding=\"utf-8-sig\")\n\n"
        image: python:3.10.7-slim
    exec-predict-mlp:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - predict_mlp
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef predict_mlp(\n    model: Input[Model],\n    features: Input[Dataset],\n\
          \    scaler_model: Input[Model],\n    pca_model: Input[Model],\n    results:\
          \ Output[Dataset],\n):\n    import pandas as pd\n    import pickle\n   \
          \ import json\n    import logging\n    import sys\n    import os\n\n   \
          \ logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    df =\
          \ pd.read_csv(features.path + \".csv\").drop('Unnamed: 0', axis = 1)\n\n\
          \    # Loading the saved model\n    model_mlp = pickle.load(open(model.path\
          \ + \".pkl\", \"rb\"))\n    scaler = pickle.load(open(scaler_model.path\
          \ + \".pkl\", \"rb\"))\n    pca = pickle.load(open(pca_model.path + \".pkl\"\
          , \"rb\"))\n\n    X_test = df.drop(\"quality\", axis=1)\n\n    # transform\
          \ the test data\n    scaled_features = pd.DataFrame(\n        scaler.transform(X_test),\n\
          \        columns=X_test.columns,\n        index=X_test.index,\n    )\n \
          \   X_test_pca = pd.DataFrame(pca.transform(scaled_features))\n\n    df_complete\
          \ = df.copy()\n    y_pred = model_mlp.predict(X_test_pca)\n    logging.info(y_pred)\n\
          \    df_complete[\"pclass\"] = y_pred.tolist()\n    df_complete.to_csv(results.path\
          \ + \".csv\", index=False, encoding=\"utf-8-sig\")\n\n"
        image: python:3.10.7-slim
    exec-train-gbc:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_gbc
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_gbc(\n    train_features_X: Input[Dataset],\n    test_features_X:\
          \ Input[Dataset],\n    train_features_y: Input[Dataset],\n    test_features_y:\
          \ Input[Dataset],\n    out_model: Output[Model],\n) -> NamedTuple(\"outputs\"\
          , metrics=dict):\n    \"\"\" \"\"\"\n    from sklearn.ensemble import GradientBoostingClassifier\n\
          \    from sklearn.metrics import (\n        accuracy_score,\n        precision_score,\n\
          \        recall_score,\n        f1_score,\n        mean_absolute_error,\n\
          \    )\n    from sklearn.model_selection import GridSearchCV\n\n    import\
          \ pandas as pd\n    import json\n    import logging\n    import sys\n  \
          \  import os\n    import pickle\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    X_train = pd.read_csv(train_features_X.path\
          \ + \".csv\")\n    y_train = pd.read_csv(train_features_y.path + \".csv\"\
          )\n\n    logging.info(X_train.columns)\n\n    parameters = {\n        \"\
          n_estimators\": [100, 250],\n        \"criterion\": [\"friedman_mse\", \"\
          squared_error\"],\n        \"min_samples_split\": [2, 5, 10],\n        \"\
          min_samples_leaf\": [1, 5, 10],\n    }\n\n    gbc = GradientBoostingClassifier(random_state=6)\n\
          \    gbc_gs = GridSearchCV(gbc, parameters)\n    gbc_gs.fit(X_train, y_train)\n\
          \    best_params = gbc_gs.best_params_\n\n    X_test = pd.read_csv(test_features_X.path\
          \ + \".csv\")\n    y_test = pd.read_csv(test_features_y.path + \".csv\"\
          )\n\n    # Predicting Test Set\n\n    y_pred = gbc_gs.predict(X_test)\n\
          \    metrics_dict = {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n\
          \        \"precision\": precision_score(y_test, y_pred, average=\"weighted\"\
          ),\n        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"\
          ),\n        \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\"\
          ),\n        \"mae\": mean_absolute_error(y_test, y_pred),\n    }\n\n   \
          \ logging.info(metrics_dict)\n\n    out_model.metadata[\"file_type\"] =\
          \ \".pkl\"\n    out_model.metadata[\"algo\"] = \"gbc_gs\"\n    out_model.metadata[\"\
          best_params\"] = best_params\n\n    # Save the model\n    m_file = out_model.path\
          \ + \".pkl\"\n    with open(m_file, \"wb\") as f:\n        pickle.dump(gbc_gs,\
          \ f)\n\n    outputs = NamedTuple(\"outputs\", metrics=dict)\n    return\
          \ outputs(metrics_dict)\n\n"
        image: python:3.10.7-slim
    exec-train-mlp:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_mlp
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_mlp(\n    train_features_X: Input[Dataset],\n    test_features_X:\
          \ Input[Dataset],\n    train_features_y: Input[Dataset],\n    test_features_y:\
          \ Input[Dataset],\n    out_model: Output[Model],\n) -> NamedTuple(\"outputs\"\
          , metrics=dict):\n    \"\"\" \"\"\"\n    from sklearn.neural_network import\
          \ MLPClassifier\n    from sklearn.metrics import (\n        accuracy_score,\n\
          \        precision_score,\n        recall_score,\n        f1_score,\n  \
          \      mean_absolute_error,\n    )\n    from sklearn.model_selection import\
          \ GridSearchCV\n\n    import pandas as pd\n    import json\n    import logging\n\
          \    import sys\n    import os\n    import pickle\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    X_train = pd.read_csv(train_features_X.path\
          \ + \".csv\")\n    y_train = pd.read_csv(train_features_y.path + \".csv\"\
          )\n\n    logging.info(X_train.columns)\n\n    parameters = {\n        \"\
          hidden_layer_sizes\": [\n            (100,),\n            (250,),\n    \
          \        (\n                100,\n                100,\n            ),\n\
          \            (\n                250,\n                250,\n           \
          \ ),\n        ],\n        \"solver\": [\"sgd\", \"adam\"],\n        \"max_iter\"\
          : [100, 300, 500],\n        \"alpha\": [0.01, 0.0001],\n    }\n\n    mlp\
          \ = MLPClassifier(random_state=6)\n    mlp_gs = GridSearchCV(mlp, parameters)\n\
          \    mlp_gs.fit(X_train, y_train)\n    best_params = mlp_gs.best_params_\n\
          \n    X_test = pd.read_csv(test_features_X.path + \".csv\")\n    y_test\
          \ = pd.read_csv(test_features_y.path + \".csv\")\n\n    # Predicting Test\
          \ Set\n\n    y_pred = mlp_gs.predict(X_test)\n    metrics_dict = {\n   \
          \     \"accuracy\": accuracy_score(y_test, y_pred),\n        \"precision\"\
          : precision_score(y_test, y_pred, average=\"weighted\"),\n        \"recall\"\
          : recall_score(y_test, y_pred, average=\"weighted\"),\n        \"f1_score\"\
          : f1_score(y_test, y_pred, average=\"weighted\"),\n        \"mae\": mean_absolute_error(y_test,\
          \ y_pred),\n    }\n\n    logging.info(metrics_dict)\n\n    out_model.metadata[\"\
          file_type\"] = \".pkl\"\n    out_model.metadata[\"algo\"] = \"mlp_gs\"\n\
          \    out_model.metadata[\"best_params\"] = best_params\n\n    # Save the\
          \ model\n    m_file = out_model.path + \".pkl\"\n    with open(m_file, \"\
          wb\") as f:\n        pickle.dump(mlp_gs, f)\n\n    outputs = NamedTuple(\"\
          outputs\", metrics=dict)\n    return outputs(metrics_dict)\n\n"
        image: python:3.10.7-slim
    exec-train-test-split:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_test_split
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_test_split(\n    dataset: Input[Dataset],\n    dataset_train_X:\
          \ Output[Dataset],\n    dataset_test_X: Output[Dataset],\n    dataset_train_y:\
          \ Output[Dataset],\n    dataset_test_y: Output[Dataset],\n    level: float,\n\
          ):\n    \"\"\"train_test_split\"\"\"\n    import pandas as pd\n    import\
          \ logging\n    import sys\n    from sklearn.model_selection import train_test_split\
          \ as tts\n\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\
          \n    alldata = pd.read_csv(dataset.path + \".csv\", index_col=None)\n \
          \   X_train, X_test, y_train, y_test = tts(\n        alldata.drop(\"quality\"\
          , axis=1),\n        alldata[\"quality\"],\n        test_size=level,\n  \
          \      random_state=6,\n    )\n    X_train.to_csv(dataset_train_X.path +\
          \ \".csv\", index=False, encoding=\"utf-8-sig\")\n    X_test.to_csv(dataset_test_X.path\
          \ + \".csv\", index=False, encoding=\"utf-8-sig\")\n    y_train.to_csv(dataset_train_y.path\
          \ + \".csv\", index=False, encoding=\"utf-8-sig\")\n    y_test.to_csv(dataset_test_y.path\
          \ + \".csv\", index=False, encoding=\"utf-8-sig\")\n\n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model]):\n    \"\"\"upload model to gcs\"\"\"\n    from google.cloud\
          \ import storage\n    import logging\n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    # upload the model to GCS\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob = bucket.blob(\n     \
          \   str(model.metadata[\"algo\"]) + \"_model\" + str(model.metadata[\"file_type\"\
          ])\n    )\n    blob.upload_from_filename(model.path + str(model.metadata[\"\
          file_type\"]))\n\n    print(\"Saved the model to GCP bucket : \" + model_repo)\n\
          \n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model]):\n    \"\"\"upload model to gcs\"\"\"\n    from google.cloud\
          \ import storage\n    import logging\n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    # upload the model to GCS\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob = bucket.blob(\n     \
          \   str(model.metadata[\"algo\"]) + \"_model\" + str(model.metadata[\"file_type\"\
          ])\n    )\n    blob.upload_from_filename(model.path + str(model.metadata[\"\
          file_type\"]))\n\n    print(\"Saved the model to GCP bucket : \" + model_repo)\n\
          \n"
        image: python:3.10.7-slim
pipelineInfo:
  name: winequality-predictor-training-pipeline-v1
root:
  dag:
    tasks:
      compare-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-compare-model
        dependentTasks:
        - train-gbc
        - train-mlp
        inputs:
          parameters:
            gbc_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-gbc
            mlp_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-mlp
        taskInfo:
          name: compare-model
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - compare-model
        - download-data-2
        - pca
        - train-mlp
        inputs:
          artifacts:
            pipelinechannel--download-data-2-dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: download-data-2
            pipelinechannel--pca-pca_model:
              taskOutputArtifact:
                outputArtifactKey: pca_model
                producerTask: pca
            pipelinechannel--pca-scaler_model:
              taskOutputArtifact:
                outputArtifactKey: scaler_model
                producerTask: pca
            pipelinechannel--train-mlp-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-mlp
          parameters:
            pipelinechannel--compare-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-model
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-1
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-model-Output']
            == 'mlp_gs'
      condition-2:
        componentRef:
          name: comp-condition-2
        dependentTasks:
        - compare-model
        - download-data-2
        - pca
        - train-gbc
        inputs:
          artifacts:
            pipelinechannel--download-data-2-dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: download-data-2
            pipelinechannel--pca-pca_model:
              taskOutputArtifact:
                outputArtifactKey: pca_model
                producerTask: pca
            pipelinechannel--pca-scaler_model:
              taskOutputArtifact:
                outputArtifactKey: scaler_model
                producerTask: pca
            pipelinechannel--train-gbc-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-gbc
          parameters:
            pipelinechannel--compare-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-model
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-2
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-model-Output']
            == 'gbc_gs'
      download-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data
        inputs:
          parameters:
            bucket:
              componentInputParameter: data_bucket
            file_name:
              componentInputParameter: dataset_filename
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: download-data
      download-data-2:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data-2
        inputs:
          parameters:
            bucket:
              componentInputParameter: data_bucket
            file_name:
              componentInputParameter: prediction_set
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: download-data-2
      pca:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-pca
        dependentTasks:
        - download-data
        inputs:
          artifacts:
            standard_features:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: download-data
          parameters:
            model_repo:
              componentInputParameter: model_repo
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: pca
      train-gbc:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-gbc
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            test_features_X:
              taskOutputArtifact:
                outputArtifactKey: dataset_test_X
                producerTask: train-test-split
            test_features_y:
              taskOutputArtifact:
                outputArtifactKey: dataset_test_y
                producerTask: train-test-split
            train_features_X:
              taskOutputArtifact:
                outputArtifactKey: dataset_train_X
                producerTask: train-test-split
            train_features_y:
              taskOutputArtifact:
                outputArtifactKey: dataset_train_y
                producerTask: train-test-split
        taskInfo:
          name: train-gbc
      train-mlp:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-mlp
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            test_features_X:
              taskOutputArtifact:
                outputArtifactKey: dataset_test_X
                producerTask: train-test-split
            test_features_y:
              taskOutputArtifact:
                outputArtifactKey: dataset_test_y
                producerTask: train-test-split
            train_features_X:
              taskOutputArtifact:
                outputArtifactKey: dataset_train_X
                producerTask: train-test-split
            train_features_y:
              taskOutputArtifact:
                outputArtifactKey: dataset_train_y
                producerTask: train-test-split
        taskInfo:
          name: train-mlp
      train-test-split:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-test-split
        dependentTasks:
        - pca
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: pca_features
                producerTask: pca
          parameters:
            level:
              runtimeValue:
                constant: 0.2
        taskInfo:
          name: train-test-split
  inputDefinitions:
    parameters:
      data_bucket:
        parameterType: STRING
      dataset_filename:
        parameterType: STRING
      model_repo:
        parameterType: STRING
      prediction_set:
        parameterType: STRING
      project_id:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.3.0
