{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41e715-3644-42de-b12a-cb82d5bce37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform\n",
    "\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bf541-efa7-40d3-bd6e-99a386f874d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    Metrics,\n",
    "    ClassificationMetrics,\n",
    "    component,\n",
    "    OutputPath,\n",
    "    InputPath,\n",
    ")\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417fa2c-3a75-4a1f-9a3c-4b1d05fd5d2f",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561d6e44-ddf9-4a48-9c23-fdaa2cd29223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"de23-398309\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://data_de2023_2065718\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ec31a-d853-4770-b2c5-0d609d98995e",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3c216d-39bb-47bb-af47-29e021aa9359",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent(\n\u001b[1;32m      2\u001b[0m     packages_to_install\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-cloud-storage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     base_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython:3.10.7-slim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_data\u001b[39m(project_id: \u001b[38;5;28mstr\u001b[39m, bucket: \u001b[38;5;28mstr\u001b[39m, file_name: \u001b[38;5;28mstr\u001b[39m, dataset: Output[Dataset]):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''download data'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storage\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    ")\n",
    "def download_data(\n",
    "    project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"download data\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Downloaing the file from a google bucket\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info(\"Downloaded Data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78d4fc-377b-4a25-a53b-dbdb54ce0a03",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ce2ca-8986-4116-85ac-569e9d9cbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(\n",
    "    dataset: Input[Dataset],\n",
    "    dataset_train_X: Output[Dataset],\n",
    "    dataset_test_X: Output[Dataset],\n",
    "    dataset_train_y: Output[Dataset],\n",
    "    dataset_test_y: Output[Dataset],\n",
    "):\n",
    "    \"\"\"train_test_split\"\"\"\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    alldata = pd.read_csv(dataset.path, index_col=None)\n",
    "    X_train, X_test, y_train, y_test = tts(\n",
    "        alldata.drop(\"quality\"), alldata[\"quality\"], test_size=0.20, random_state=6\n",
    "    )\n",
    "    X_train.to_csv(dataset_train_X.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    X_test.to_csv(dataset_test_X.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    y_train.to_csv(dataset_train_y.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    y_test.to_csv(dataset_test_y.path + \".csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1c694-12e1-4581-b3ac-ea8c94380527",
   "metadata": {},
   "source": [
    "#### Pipeline Component : PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7a8e8-a834-48a3-b4cb-d8870103ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def PCA(standard_features: Input[Dataset], pca_features: Output[Dataset]):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    train_features = pd.read_csv(standard_features.path + \".csv\")\n",
    "    scaled_features = pd.DataFrame(\n",
    "        scaler.fit_transform(train_features),\n",
    "        columns=train_features.columns,\n",
    "        index=train_features.index,\n",
    "    )\n",
    "\n",
    "    pca = PCA(n_components=0.1, svd_solver=\"full\")\n",
    "    pca_df = pca.fit_transform(scaled_features)\n",
    "\n",
    "    pca_df.to_csv(pca_features.path + \".csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b8dd4-6b1c-4cca-ac6a-e3aa2f30dfb5",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd1aa34-7b2d-45f6-ae24-12fa8cfcb378",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1858410057.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 33\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"n_estimators\": [50, 100, 250]\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_rf(\n",
    "    train_features_X: Input[Dataset],\n",
    "    test_features_X: Input[Dataset],\n",
    "    train_features_y: Input[Dataset],\n",
    "    test_features_y: Input[Dataset],\n",
    "    out_model: Output[Model],\n",
    ") -> NamedTuple(\"outputs\", metrics=dict):\n",
    "    \"\"\" \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    X_train = pd.read_csv(train_features_X.path + \".csv\")\n",
    "    y_train = pd.read_csv(train_features_y.path + \".csv\")\n",
    "\n",
    "    logging.info(X_train.columns)\n",
    "\n",
    "    parameters = {\n",
    "        \"n_estimators\": [100, 250],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 5, 10],\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=6)\n",
    "    rf_gs = GridSearchCV(rf, parameters)\n",
    "    rf_gs.fit(X_train, y_train)\n",
    "    best_params = rf_gs.best_params_\n",
    "\n",
    "    X_test = pd.read_csv(test_features_X.path + \".csv\")\n",
    "    y_test = pd.read_csv(test_features_y.path + \".csv\")\n",
    "\n",
    "    # Predicting Test Set\n",
    "\n",
    "    y_pred = rf_gs.predict(X_test)\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "    logging.info(metrics_dict)\n",
    "\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"rf_gs\"\n",
    "    out_model.metadata[\"best_params\"] = best_params\n",
    "\n",
    "    # Save the model\n",
    "    m_file = out_model.path + \".pkl\"\n",
    "    with open(m_file, \"wb\") as f:\n",
    "        pickle.dump(rf_gs, f)\n",
    "\n",
    "    outputs = NamedTuple(\"outputs\", metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac202838-a61d-4806-ae58-3ea2dc79760e",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-GBC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81315be-31d9-4b63-8105-ad26b3bfa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_gbc(\n",
    "    train_features_X: Input[Dataset],\n",
    "    test_features_X: Input[Dataset],\n",
    "    train_features_y: Input[Dataset],\n",
    "    test_features_y: Input[Dataset],\n",
    "    out_model: Output[Model],\n",
    ") -> NamedTuple(\"outputs\", metrics=dict):\n",
    "    \"\"\" \"\"\"\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    X_train = pd.read_csv(train_features_X.path + \".csv\")\n",
    "    y_train = pd.read_csv(train_features_y.path + \".csv\")\n",
    "\n",
    "    logging.info(X_train.columns)\n",
    "\n",
    "    parameters = {\n",
    "        \"n_estimators\": [100, 250],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 5, 10],\n",
    "    }\n",
    "\n",
    "    gbc = GradientBoostingClassifier(random_state=6)\n",
    "    gbc_gs = GridSearchCV(gbc, parameters)\n",
    "    gbc_gs.fit(X_train, y_train)\n",
    "    best_params = gbc_gs.best_params_\n",
    "\n",
    "    X_test = pd.read_csv(test_features_X.path + \".csv\")\n",
    "    y_test = pd.read_csv(test_features_y.path + \".csv\")\n",
    "\n",
    "    # Predicting Test Set\n",
    "\n",
    "    y_pred = gbc_gs.predict(X_test)\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"micro\"),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "    logging.info(metrics_dict)\n",
    "\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"gbc_gs\"\n",
    "    out_model.metadata[\"best_params\"] = best_params\n",
    "\n",
    "    # Save the model\n",
    "    m_file = out_model.path + \".pkl\"\n",
    "    with open(m_file, \"wb\") as f:\n",
    "        pickle.dump(gbc_gs, f)\n",
    "\n",
    "    outputs = NamedTuple(\"outputs\", metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06696dbd",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_rf(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "\n",
    "    filename = model.path + \".pkl\"\n",
    "\n",
    "    # Loading the saved model\n",
    "    model_rf = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "    X_test = df.drop(columns=[\"quality\"])\n",
    "\n",
    "    df_complete = df.copy()\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    logging.info(y_pred)\n",
    "    df_complete[\"pclass\"] = y_pred.tolist()\n",
    "    df_complete.to_csv(results.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91997764",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_rf(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "\n",
    "    filename = model.path + \".pkl\"\n",
    "\n",
    "    # Loading the saved model\n",
    "    model_gbc = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "    X_test = df.drop(columns=[\"quality\"])\n",
    "\n",
    "    df_complete = df.copy()\n",
    "    y_pred = model_gbc.predict(X_test)\n",
    "    logging.info(y_pred)\n",
    "    df_complete[\"pclass\"] = y_pred.tolist()\n",
    "    df_complete.to_csv(results.path + \".csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92339315",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905da275",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10.7-slim\")\n",
    "def compare_model(rf_metrics: dict, gbc_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(rf_metrics)\n",
    "    logging.info(gbc_metrics)\n",
    "    if rf_metrics.get(\"f1_score\") > gbc_metrics.get(\"f1_score\"):\n",
    "        return \"rf\"\n",
    "    else:\n",
    "        return \"gbc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d8cb7",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"], base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    \"\"\"upload model to gsc\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob(\n",
    "        str(model.metadata[\"algo\"]) + \"_model\" + str(model.metadata[\"file_type\"])\n",
    "    )\n",
    "    blob.upload_from_filename(model.path + str(model.metadata[\"file_type\"]))\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f21478",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(name=\"winequality-predictor-training-pipeline-v1\")\n",
    "def pipeline(project_id: str, data_bucket: str, data_filename: str, model_repo: str):\n",
    "    di_op = download_data(\n",
    "        project_id=project_id, bucket=data_bucket, file_name=trainset_filename\n",
    "    )\n",
    "\n",
    "    training_mlp_job_run_op = train_mlp(features=di_op.outputs[\"dataset\"])\n",
    "\n",
    "    training_lr_job_run_op = train_lr(features=di_op.outputs[\"dataset\"])\n",
    "\n",
    "    pre_di_op = download_data(\n",
    "        project_id=project_id, bucket=data_bucket, file_name=testset_filename\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)\n",
    "\n",
    "    comp_model__op = compare_model(\n",
    "        mlp_metrics=training_mlp_job_run_op.outputs[\"metrics\"],\n",
    "        lr_metrics=training_lr_job_run_op.outputs[\"metrics\"],\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)\n",
    "\n",
    "    # defining the branching condition\n",
    "    with dsl.If(comp_model__op.output == \"MLP\"):\n",
    "        predict_mlp_job_run_op = predict_mlp(\n",
    "            model=training_mlp_job_run_op.outputs[\"out_model\"],\n",
    "            features=pre_di_op.outputs[\"dataset\"],\n",
    "        )\n",
    "        upload_model_mlp_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_mlp_job_run_op.outputs[\"out_model\"],\n",
    "        ).after(predict_mlp_job_run_op)\n",
    "\n",
    "    with dsl.If(comp_model__op.output == \"LR\"):\n",
    "        predict_lr_job_run_op = predict_lr(\n",
    "            model=training_lr_job_run_op.outputs[\"out_model\"],\n",
    "            features=pre_di_op.outputs[\"dataset\"],\n",
    "        )\n",
    "        upload_model_lr_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_job_run_op.outputs[\"out_model\"],\n",
    "        ).after(predict_lr_job_run_op)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
